---
title: "p8105_hw2_ns3782"
author: "NSK"
date: "2024-09-26"
output: github_document
toc: true
code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries, include=FALSE}
library(tidyverse)
library(readxl)

```

# Problem 1

```{r problem_1}

# Read and clean NYC transit data

nyc_transit_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", ".", "")) %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, route1:route11, entry, vending, entrance_type, ada) %>%
  mutate(entry = case_match(
    entry,
    "YES" ~ TRUE,
    "NO" ~ FALSE,
    .default = NA
  ))

```

## Dataset overview

The cleaned NYC Transit Subway Entrance and Exit dataset contains information about entrances and exits of subway stations in New York City. Variables selected for inclusion in the cleaned dataset imported into R include the line, station name, location coordinates (latitude and longitude), routes served (route 1 through to route 11), entry, vending presence, entrance type, and ADA compliance. 

The data cleaning process involved reading the CSV file from a local subdirectory /data, cleaning variable names using the janitor package, selecting variables of interest, and converting the 'entry' variable from character ("YES"/"NO") to logical (TRUE/FALSE). 

The resulting dataset has **`r nrow(nyc_transit_df)` rows** and **`r ncol(nyc_transit_df)` columns**. The dataset is not tidy because information about routes served is spread across multiple columns (Route1 through Route11). This does not align with good tidy data practice, as each variable should form a column and each observation should form a row.

## How many distinct stations are there?

```{r distinct_stations}

nyc_transit_df %>%
  distinct(station_name, line) %>%
  nrow()

```

There are `r nyc_transit_df %>% distinct(station_name, line) %>% nrow()` distinct subway stations in the NYC transit subway system.


## How many stations are ADA compliant?

```{r ada}

nyc_transit_df %>%
  filter(ada == TRUE) %>%
  distinct(station_name, line) %>%
  nrow()

```

There are `r nyc_transit_df %>% filter(ada == TRUE) %>% distinct(station_name, line) %>% nrow()` ADA compliant stations.


## What proportion of station entrances / exits without vending allow entrance?

```{r allow_entry}

prop_no_vending_entry <- nyc_transit_df %>%
  filter(vending == "NO") %>%
  summarise(prop = mean(entry, na.rm = TRUE)) %>%
  pull(prop)

```

The proportion of station entrances/exits without vending that allow entrance is `r prop_no_vending_entry`, or `r scales::percent(prop_no_vending_entry, accuracy = 0.1)`.


# Problem 2

```{r problem_2}

# Read and clean Mr. Trash Wheel sheet

mr_trash_wheel_df <- read_excel(
  "./data/trash_wheel_collection_data.xlsx",
  sheet = "Mr. Trash Wheel",
  range = cell_cols("A:N"),
  skip = 1
) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(
    sports_balls = as.integer(round(sports_balls)),
    trash_wheel = "Mr. Trash Wheel",
    year = as.numeric(year)
  )

# Read and clean Professor Trash Wheel sheet

prof_trash_wheel_df <- read_excel(
  "./data/trash_wheel_collection_data.xlsx",
  sheet = "Professor Trash Wheel",
  range = cell_cols("A:M"),
  skip = 1
) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(
    trash_wheel = "Professor Trash Wheel",
    year = as.numeric(year)
  )

# Read and clean Gwynnda Wheel sheet

gwynnda_trash_wheel_df <- read_excel(
  "./data/trash_wheel_collection_data.xlsx",
  sheet = "Gwynnda Trash Wheel",
  range = cell_cols("A:L"),
  skip = 1
) %>% 
  janitor::clean_names() %>% 
  filter(!is.na(dumpster)) %>% 
  mutate(
    trash_wheel = "Gwynnda Trash Wheel",
    year = as.numeric(year)
  )


# Combine all trash wheel datasets

combined_trash_wheel <- bind_rows(
  mr_trash_wheel_df, 
  prof_trash_wheel_df, 
  gwynnda_trash_wheel_df
) %>%
  select(trash_wheel, everything())



# Total weight collected by Prof Trash Wheel

prof_total_weight <- prof_trash_wheel_df %>%
  summarize(total_weight = sum(weight_tons, na.rm = TRUE)) %>%
  pull(total_weight)

# Total cigarette butts collected by Gwynnda June 2022

gwynnda_june_2022_butts <- gwynnda_trash_wheel_df %>%
  filter(month == "June", year == 2022) %>%
  summarize(total_butts = sum(cigarette_butts, na.rm = TRUE)) %>%
  pull(total_butts)

```

## Combined trash wheel Dataset Overview

The combined Trash Wheel dataset contains `r nrow(combined_trash_wheel)` observations and `r ncol(combined_trash_wheel)` variables. Key variables include the identifier for each trash wheel (`r paste(unique(combined_trash_wheel$trash_wheel), collapse = ", ")`), `r paste(setdiff(names(combined_trash_wheel), "trash_wheel"), collapse = ", ")`.

The data shows that Professor Trash Wheel has collected a total of `r prof_total_weight` tons of trash since its deployment. In June 2022, Gwynnda Trash Wheel collected `r scales::comma(gwynnda_june_2022_butts, accuracy = 1)` cigarette butts.

# Problem 3

## Importing and Cleaning datasets.

```{r p3_import_clean}

# Import and clean results csv

results_df <- read_csv("./data/results.csv", skip = 2) %>% 
  janitor::clean_names() %>%
  rename(baker_first_name = baker)

# Import and clean results bakers csv

bakers_df <- read_csv("./data/bakers.csv") %>% 
  janitor::clean_names() %>%
  separate(baker_name, into = c("baker_first_name", "baker_last_name"), sep = " ", extra = "merge")

# Import and clean results bakes csv

bakes_df <- read_csv("./data/bakes.csv") %>% 
  janitor::clean_names() %>%
  rename(baker_first_name = baker) %>%
  mutate(across(where(is.character), ~case_match(
    .,
    c("Unknown", "UNKNOWN", "N/A") ~ NA_character_,
    .default = . %>%
      str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
      str_replace_all("(?<=[a-z])with", " with") %>%
      str_replace_all("\\s+", " ") %>%
      str_trim()
  )),
  baker_first_name = str_remove_all(baker_first_name, "\""))  # Removes the quotes from "Jo"

# Import and clean viewers csv

viewers_df <- read_csv("./data/viewers.csv") %>% 
  janitor::clean_names()

# Pivoting viewers_df into long format

viewers_long_df <- viewers_df %>%
  pivot_longer(
    cols = starts_with("series_"),
    names_to = "series",
    values_to = "viewers",
    names_prefix = "series_"
  ) %>%
  mutate(series = as.numeric(series)) %>%
  select(series, everything())

```


## Checking for Incomplete Data between bakers_df and bakes_df

```{r p3_incomplete_data}

# Check for bakers without any competition data

missing_bakers <- bakers_df %>%
  anti_join(bakes_df, by = c("baker_first_name", "series"))
print(missing_bakers)

# Check for bakes information without baker information

missing_bakes <- bakes_df %>%
  anti_join(bakers_df, by = c("baker_first_name", "series"))
print(missing_bakes)

```

There are 25 bakers from bakers_df without bakes information (bakes_df).
There is no bakes data from bakes_df with missing bakers information (bakers_df).

## Merging, cleaning, and exporting final dataset

```{r p3_merging_data}

# Merging bakers_df and bakes_df 

bakers_bake_merged_df <- full_join(bakes_df, bakers_df, 
                                   by = c("baker_first_name", "series"), 
                                   suffix = c("", "_baker")) %>%
    select(baker_first_name, baker_last_name, series, episode, baker_age, baker_occupation, hometown, everything())


# Check for results information missing from bakers_bake_merged_df

missing_results <- results_df %>%
  anti_join(bakers_bake_merged_df, by = c("baker_first_name", "series"))
print(missing_results)

# Correcting baker_first_name discrepancy between datasets

bakers_bake_merged_df <- bakers_bake_merged_df %>%
  mutate(baker_first_name = if_else(baker_first_name == "Jo", "Joanne", baker_first_name))

# Merging bakers_bake_merged_df and results_df 

bakers_bake_merged_df <- bakers_bake_merged_df %>%
  left_join(results_df, by = c("baker_first_name", "series", "episode"))

# Export the merged dataset

write_csv(bakers_bake_merged_df, "./data/gbbo_merged_data.csv")

# Identify potential duplicates

duplicates <- bakers_bake_merged_df %>%
  group_by(series, episode, baker_first_name) %>%
  filter(n() > 1) %>%
  arrange(series, episode, baker_first_name)


```

## Data Cleaning Process

The first step in my data wrangling process involved importing and cleaning all 4 datasets. I used the janitor package to convert column names into snake case. For the results.csv, the first two rows were skipped to avoid a header in the data.
For some datasets, data cleaning and wrangling required extra steps. For instance, with bakers.csv, I decided to split the baker names into first and last names. This ensured that variables across multiple datasets were consistent and appropriately defined in preparation for merging datasets later on. In the case of bakes.csv, I also had some minor formatting to do to address trailing whitespace, incorrect punctuation in string characters, and ensuring correct spacing between words.
Viewers.csv was not in tidy format. As a result, I had to pivot it into long format for it to be tidy, with each variable forming a column and each observation forming a row.

As part of my data cleaning process, I checked for inconsistent/missing data across datasets. Anti-joins were used to check for bakers without competition data and bakes without baker information.
From this, I found that there were 25 bakers from bakers_df without bakes information (bakes_df).
There was no bakes data from bakes_df with missing bakers information (bakers_df).

While I had the opportunity to remove the 25 bakers from the dataset, I chose not to so as to preserve as much information as possible. These bakers might represent contestants who were selected but didn't participate in any bakes, or there might be missing data for their bakes. Keeping this information could be valuable for future analyses or for understanding the completeness of the dataset.

Questions I had were:

* Why are there 25 bakers without bakes information? Are these from specific seasons or years?
* Should I include these bakers in the final merged dataset, even though they lack bake information?
* How should I handle potential discrepancies in baker names across datasets?

After careful consideration, I decided to keep all bakers in the dataset and use a full join when merging bakers_df and bakes_df to ensure no information was lost. First, I used full_join on baker_first_name and series to merge bakes_df and bakers_df in order to preserve all information across both datasets. As expected based on the check I did previously, this dataset bakers_bake_merged_df had 25 observations (bakers) without bakes information.

Next, I checked for inconsistent/missing data across datasets between the merged dataset and results_df. There were 8 observations (bakers) in the results_df that did not appear in the merged dataset. However, upon observation, this issue was a name discrepancy. In results_df, the baker was named "Joanne", however in bakers_bake_merged_df, the same baker is named "Jo". I confirmed that these were the same individual by looking up contestant information details of the Great British Bakeoff season 2 online.

In order to rectify this discrepancy, I renamed "Jo" from bakers_bake_merged_df to "Joanne" to be consistent across datasets.
I then merged results_df with bakers_bake_merged_df to create the final Great British Bakeoff dataset.
After merging, I performed a check for duplicates, looking at whether a baker might have multiple entries for the same episode and series. I also checked whether the merged dataset contained all the necessary variables from each of the original datasets.

I then exported this data as a CSV in the appropriate local directory.


## Great British Bakeoff: Final dataset description

The final merged GBBO dataset provides a comprehensive view of the competition across multiple seasons. The dataset contains `r nrow(bakers_bake_merged_df)` rows and `r ncol(bakers_bake_merged_df)` columns. It includes data for `r length(unique(bakers_bake_merged_df$baker_first_name))` unique contestants across `r length(unique(bakers_bake_merged_df$series))` seasons of the show. 
It provides additional information about each contestant, including their age, occupation and hometown, their baking, and their performance throughout the competition.
The ages of contestants range from `r min(bakers_bake_merged_df$baker_age, na.rm = TRUE)` to `r max(bakers_bake_merged_df$baker_age, na.rm = TRUE)` years old, with an average age of `r round(mean(bakers_bake_merged_df$baker_age, na.rm = TRUE), 1)` years.

## Season Star Bakers and Overview

```{r p3_winners_outcome}

star_bakers <- bakers_bake_merged_df %>%
  filter(series >= 5 & series <= 10, 
         result %in% c("STAR BAKER", "WINNER")) %>%
  select(series, episode, baker_first_name, result) %>%
  arrange(series, episode)

# Create readable table

knitr::kable(star_bakers, 
             caption = "Star Bakers and Winners (Seasons 5-10)",
             col.names = c("Season", "Episode", "Baker", "Result"))

```

In season 5, Nancy was the dark horse of the competition - she only received the Star Baker award once in the first episode, but won the entire episode at the end of the season.
In season 6, it seemed a bit more predictable towards the later half of the series. While Ian seemed strong for the first few episodes, winning 3 Star Baker awards until episode 4, Nadiya began to outperform him in later episodes - garnering 3 Star Baker  awards in the latter part of the series, and winning the competition overall.
Season 7 seemed harder to predict a clear winner early on, however Candice won the most Star Baker awards before winning the whole competition.
In Season 8, Steven did consistently well, earning two Star Baker awards early on in the season and three overall. However, Sophie secured the win unexpectedly later on in the season, even though she only won two Star Baker awards overall.

## Viewers Data

```{r p3_viewers}

# Show the first 10 rows of tidy viewers data

head(viewers_long_df, 10)

# Average viewership Season 1

mean_viewer_s1 <- viewers_long_df %>%
  filter(series == 1) %>%
  summarise(average_viewership = mean(viewers, na.rm = TRUE))

# Average viewership Season 5

mean_viewer_s5 <- viewers_long_df %>%
  filter(series == 5) %>%
  summarise(average_viewership = mean(viewers, na.rm = TRUE))


```

The average viewership in season 1 was `r mean_viewer_s1` million, increasing to an average viewership of `r mean_viewer_s5` million in season 5.
